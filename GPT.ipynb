{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mastodon import Mastodon\n",
    "\n",
    "# 初始化Mastodon客户端\n",
    "mastodon_instance_url = \"https://mastodon.social/\"\n",
    "mastodon = Mastodon(api_base_url=mastodon_instance_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Politics -----------\n",
      "                          NationalPolitics\n",
      "                          Elections2023\n",
      "                          ForeignAffairs\n",
      "                          LawsAndRegulations\n",
      "                          LocalGovernance\n",
      "----------- Technology -----------\n",
      "                          Computing\n",
      "                          TechNews\n",
      "                          Gadgets\n",
      "                          Programming\n",
      "                          ArtificialIntelligence\n",
      "----------- Science -----------\n",
      "                          Biology\n",
      "                          QuantumPhysics\n",
      "                          OrganicChemistry\n",
      "                          Astronomy\n",
      "                          MedicalResearch\n",
      "----------- Entertainment -----------\n",
      "                          Movies\n",
      "                          Music\n",
      "                          VideoGames\n",
      "                          Theatre\n",
      "                          PopCulture\n",
      "----------- News -----------\n",
      "                          TopHeadlines\n",
      "                          BreakingNews\n",
      "                          Reportage\n",
      "                          WorldUpdates\n",
      "                          CitizenJournalism\n",
      "----------- Art and Culture -----------\n",
      "                          ModernArt\n",
      "                          ClassicLiterature\n",
      "                          MuseumVisits\n",
      "                          IndependentCinema\n",
      "                          ArtHistory\n",
      "----------- Sports -----------\n",
      "                          Football\n",
      "                          Basketball\n",
      "                          OlympicGames\n",
      "                          FavoriteTeams\n",
      "                          SportsTraining\n",
      "----------- Health -----------\n",
      "                          MentalHealth\n",
      "                          HolisticMedicine\n",
      "                          PhysicalWellness\n",
      "                          Women'sHealth\n",
      "                          MedicalResearch\n",
      "----------- Environment -----------\n",
      "                          ClimateChange\n",
      "                          Sustainability\n",
      "                          NatureProtection\n",
      "                          RenewableEnergy\n",
      "                          UrbanEcology\n",
      "----------- Travel -----------\n",
      "                          ExoticDestinations\n",
      "                          TravelTips\n",
      "                          TravelExperiences\n",
      "                          OutdoorAdventures\n",
      "                          LocalTourism\n",
      "----------- Fashion -----------\n",
      "                          FashionTrends\n",
      "                          HauteCouture\n",
      "                          FashionAccessories\n",
      "                          StreetStyles\n",
      "                          FashionEthics\n",
      "----------- Education -----------\n",
      "                          HigherEducation\n",
      "                          OnlineLearning\n",
      "                          LocalSchools\n",
      "                          InnovativePedagogy\n",
      "                          ProfessionalTraining\n",
      "----------- Food -----------\n",
      "                          CookingRecipes\n",
      "                          HealthyCuisine\n",
      "                          Gastronomy\n",
      "                          LocalRestaurants\n",
      "                          WorldCuisines\n",
      "----------- Humor -----------\n",
      "                          Jokes\n",
      "                          StandUpComedy\n",
      "                          FunnyMemes\n",
      "                          Parodies\n",
      "                          SatiricalComedy\n",
      "----------- Lifestyle -----------\n",
      "                          Wellness\n",
      "                          HealthyHabits\n",
      "                          PersonalDevelopment\n",
      "                          FamilyAndParenting\n",
      "                          BalancedLife\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary library\n",
    "import json\n",
    "\n",
    "# Read the dictionary from the text file\n",
    "with open('topic_dictionary.txt', 'r') as file:\n",
    "    topic_hashtags = json.load(file)\n",
    "\n",
    "for topic in topic_hashtags.keys():\n",
    "    print('-----------',topic,\"-----------\")\n",
    "    for hashtag in topic_hashtags[topic].keys():\n",
    "        print('                         ',hashtag)\n",
    "        list_dict = extract_mastodon(mastodon, hashtag,50 )\n",
    "        topic_hashtags[topic][hashtag]=list_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtag_related 1</th>\n",
       "      <th>hashtag_related 2</th>\n",
       "      <th>hashtag_related 3</th>\n",
       "      <th>hashtag_related 4</th>\n",
       "      <th>hashtag_related 5</th>\n",
       "      <th>author</th>\n",
       "      <th>create_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Politics</td>\n",
       "      <td>NationalPolitics</td>\n",
       "      <td>European socialists suspend Robert Fico’s Smer...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>politico_eu_bot</td>\n",
       "      <td>2023-10-12 16:34:46.456000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Politics</td>\n",
       "      <td>NationalPolitics</td>\n",
       "      <td>Now, a member of Minnesota's Congressional del...</td>\n",
       "      <td>#Business</td>\n",
       "      <td>#Hudson</td>\n",
       "      <td>#UAWStrike</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>strike</td>\n",
       "      <td>2023-10-06 11:52:06+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politics</td>\n",
       "      <td>NationalPolitics</td>\n",
       "      <td>Markus Söder’s crumbling empire https://www.po...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>politico_eu_bot</td>\n",
       "      <td>2023-10-06 02:18:11.689000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Politics</td>\n",
       "      <td>NationalPolitics</td>\n",
       "      <td>Poland, Hungary, Slovakia impose own Ukraine g...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>politico_eu_bot</td>\n",
       "      <td>2023-09-16 09:34:38.494000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Politics</td>\n",
       "      <td>NationalPolitics</td>\n",
       "      <td>Vienna seeks to calm Selmayr ‘blood money’ fur...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>politico_eu_bot</td>\n",
       "      <td>2023-09-10 17:33:43.008000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>BalancedLife</td>\n",
       "      <td>Wishing you a day filled with balance, positiv...</td>\n",
       "      <td>#WorkPlayTravel</td>\n",
       "      <td>#GoodDayAhead</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>workplaytravel</td>\n",
       "      <td>2023-11-01 19:55:15.454000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>BalancedLife</td>\n",
       "      <td>Embrace calm with Ashwagandha 🌿✨ by Moodbeli i...</td>\n",
       "      <td>#PlantiaWellness</td>\n",
       "      <td>#AshwagandhaMagic</td>\n",
       "      <td>#StressRelief</td>\n",
       "      <td>#ImmuneBoost</td>\n",
       "      <td>None</td>\n",
       "      <td>Plantia</td>\n",
       "      <td>2023-10-19 19:40:02.490000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>BalancedLife</td>\n",
       "      <td>Sheesh - was I ever that young?? Please check ...</td>\n",
       "      <td>#Novel</td>\n",
       "      <td>#deathspaleflag</td>\n",
       "      <td>#psychologicalthriller</td>\n",
       "      <td>#medicine</td>\n",
       "      <td>#neurosurgeon</td>\n",
       "      <td>GaryRSimonds</td>\n",
       "      <td>2023-06-28 15:32:31.069000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>BalancedLife</td>\n",
       "      <td># StressManagement # BalancedLife # Mindfulnes...</td>\n",
       "      <td>#StressManagement</td>\n",
       "      <td>#MindfulnessTips</td>\n",
       "      <td>#SelfCare</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Clusterado</td>\n",
       "      <td>2023-06-19 12:29:29.076000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>BalancedLife</td>\n",
       "      <td>@ rickmans slow is best. We should learn how t...</td>\n",
       "      <td>#Zen</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>adrianan</td>\n",
       "      <td>2022-12-18 14:25:06.824000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2095 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          topic           hashtag  \\\n",
       "0      Politics  NationalPolitics   \n",
       "1      Politics  NationalPolitics   \n",
       "2      Politics  NationalPolitics   \n",
       "3      Politics  NationalPolitics   \n",
       "4      Politics  NationalPolitics   \n",
       "...         ...               ...   \n",
       "2090  Lifestyle      BalancedLife   \n",
       "2091  Lifestyle      BalancedLife   \n",
       "2092  Lifestyle      BalancedLife   \n",
       "2093  Lifestyle      BalancedLife   \n",
       "2094  Lifestyle      BalancedLife   \n",
       "\n",
       "                                                content  hashtag_related 1  \\\n",
       "0     European socialists suspend Robert Fico’s Smer...               None   \n",
       "1     Now, a member of Minnesota's Congressional del...          #Business   \n",
       "2     Markus Söder’s crumbling empire https://www.po...               None   \n",
       "3     Poland, Hungary, Slovakia impose own Ukraine g...               None   \n",
       "4     Vienna seeks to calm Selmayr ‘blood money’ fur...               None   \n",
       "...                                                 ...                ...   \n",
       "2090  Wishing you a day filled with balance, positiv...    #WorkPlayTravel   \n",
       "2091  Embrace calm with Ashwagandha 🌿✨ by Moodbeli i...   #PlantiaWellness   \n",
       "2092  Sheesh - was I ever that young?? Please check ...             #Novel   \n",
       "2093  # StressManagement # BalancedLife # Mindfulnes...  #StressManagement   \n",
       "2094  @ rickmans slow is best. We should learn how t...               #Zen   \n",
       "\n",
       "      hashtag_related 2       hashtag_related 3 hashtag_related 4  \\\n",
       "0                  None                    None              None   \n",
       "1               #Hudson              #UAWStrike              None   \n",
       "2                  None                    None              None   \n",
       "3                  None                    None              None   \n",
       "4                  None                    None              None   \n",
       "...                 ...                     ...               ...   \n",
       "2090      #GoodDayAhead                    None              None   \n",
       "2091  #AshwagandhaMagic           #StressRelief      #ImmuneBoost   \n",
       "2092    #deathspaleflag  #psychologicalthriller         #medicine   \n",
       "2093   #MindfulnessTips               #SelfCare              None   \n",
       "2094               None                    None              None   \n",
       "\n",
       "     hashtag_related 5           author                        create_at  \n",
       "0                 None  politico_eu_bot 2023-10-12 16:34:46.456000+00:00  \n",
       "1                 None           strike        2023-10-06 11:52:06+00:00  \n",
       "2                 None  politico_eu_bot 2023-10-06 02:18:11.689000+00:00  \n",
       "3                 None  politico_eu_bot 2023-09-16 09:34:38.494000+00:00  \n",
       "4                 None  politico_eu_bot 2023-09-10 17:33:43.008000+00:00  \n",
       "...                ...              ...                              ...  \n",
       "2090              None   workplaytravel 2023-11-01 19:55:15.454000+00:00  \n",
       "2091              None          Plantia 2023-10-19 19:40:02.490000+00:00  \n",
       "2092     #neurosurgeon     GaryRSimonds 2023-06-28 15:32:31.069000+00:00  \n",
       "2093              None       Clusterado 2023-06-19 12:29:29.076000+00:00  \n",
       "2094              None         adrianan 2022-12-18 14:25:06.824000+00:00  \n",
       "\n",
       "[2095 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your nested dictionary\n",
    "# author +  create_at + contents + hashtag_related\n",
    "# Initialize an empty list to store records\n",
    "records = []\n",
    "\n",
    "# Flatten the nested dictionary into records\n",
    "for topic, hashtags in topic_hashtags.items():\n",
    "    for hashtag, entries in hashtags.items():\n",
    "        for entry in entries:\n",
    "            records.append({\n",
    "                \"topic\": topic,\n",
    "                \"hashtag\": hashtag,\n",
    "                \"content\": entry[\"content\"],\n",
    "                \"hashtag_related 1\": entry[\"related_topics\"][0],\n",
    "                \"hashtag_related 2\": entry[\"related_topics\"][1],\n",
    "                \"hashtag_related 3\": entry[\"related_topics\"][2],\n",
    "                \"hashtag_related 4\": entry[\"related_topics\"][3],\n",
    "                \"hashtag_related 5\": entry[\"related_topics\"][4],\n",
    "                \"author\": entry[\"author\"],\n",
    "                \"create_at\": entry[\"created_at\"],\n",
    "            })\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(\"15topics.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "European socialists suspend Robert Fico’s Smer party and its ally Hlas    \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Example data preparation (replace with your own data loading and processing)\n",
    "texts = df[\"content\"].tolist()\n",
    "#delete url of texts\n",
    "texts = [re.sub(r\"http\\S+\", \"\", text) for text in texts]\n",
    "\n",
    "labels = df[\"topic\"].tolist()\n",
    "topic_to_num = {topic :index for index,topic in enumerate(topic_hashtags.keys())}\n",
    "labels_num = [topic_to_num[label] for label in labels]\n",
    "print(texts[0])\n",
    "print(labels_num[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the texts\n",
    "import torch\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "encoding = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "labels_num = torch.tensor(labels_num)\n",
    "print(input_ids.shape, labels_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainset and testset\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "dataset = TensorDataset(input_ids, labels_num)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "#trainloader and testloader\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset,sampler = RandomSampler(train_dataset),batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_dataset,sampler = SequentialSampler(test_dataset),batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 15  # Replace with your actual number of classes\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, num_classes)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "#for epoch in range(5):\n",
    "for batch in train_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids, labels = batch\n",
    "    print(input_ids, labels)\n",
    "    print(input_ids.shape, labels.shape)\n",
    "    outputs = model(input_ids=input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch[0]\n",
    "        labels = batch[1]\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs[0]\n",
    "        predictions.extend(torch.argmax(logits, dim=-1).tolist())\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels_num[train_size:], predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine_tuned_gpt2_model\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gpt2_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = GPT2ForSequenceClassification.from_pretrained(\"fine_tuned_gpt2_model\")\n",
    "loaded_tokenizer = GPT2Tokenizer.from_pretrained(\"fine_tuned_gpt2_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
